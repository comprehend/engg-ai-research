{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f96e4ab",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df24be3-75ff-49b5-83a8-a8d4ae1be983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForTokenClassification, ElectraTokenizerFast\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de96b7ee",
   "metadata": {},
   "source": [
    "### MODEL CONFIGURATION DETAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3d1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"pretrained_models/\"\n",
    "MODEL_PATH = BASE_PATH + \"NegBioElectra/\"\n",
    "LR_RATE = 1e-5\n",
    "CKPT_PATH = BASE_PATH + \"NegBioElectra_sherlock_cue_model\"\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49011670",
   "metadata": {},
   "source": [
    "### LOAD BASE MODEL & TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eb49558-a4c3-4b1e-9052-b3c01a5cb130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraForTokenClassification: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFElectraForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFElectraForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_PATH, num_labels=3,from_pt=True\n",
    ")\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(MODEL_PATH,max_length=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45102349",
   "metadata": {},
   "source": [
    "### LOAD DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a0fb09-dbaa-4eef-967a-6a73b2fb387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a249b2b4cc520fbf\n",
      "Reusing dataset csv (/home/studio-lab-user/.cache/huggingface/datasets/csv/default-a249b2b4cc520fbf/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05cc45266f074213b32eb7d946e669b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'labels'],\n",
       "        num_rows: 847\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['sentence', 'labels'],\n",
       "        num_rows: 144\n",
       "    })\n",
       "    test_card: Dataset({\n",
       "        features: ['sentence', 'labels'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "    test_circle: Dataset({\n",
       "        features: ['sentence', 'labels'],\n",
       "        num_rows: 116\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"csv\", data_files={'train': 'sher_m1_train.csv','dev':'sher_m1_dev.csv',\"test_card\":\"sher_m1_test_cardboard.csv\",\"test_circle\":\"sher_m1_test_circle.csv\"})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31681b26",
   "metadata": {},
   "source": [
    "### PRE-PROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fd2ce72-2086-4376-b454-893bbeeea650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(examples):\n",
    "    l = {'labels':[list(np.array(i.split(\"|\"),dtype=\"int32\")) for i in examples['labels']]} \n",
    "    l.update(tokenizer(examples['sentence'],truncation=True,max_length=256,padding=True))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86f9cbb-f676-44f9-8ccb-f456d61079bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/csv/default-a249b2b4cc520fbf/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-eeda3117e9604fc9.arrow\n",
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/csv/default-a249b2b4cc520fbf/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-2898f1a61108b0ee.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af6cb8d73d04ddaa2aa3112d48bbfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/csv/default-a249b2b4cc520fbf/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-71e6d4cb0e20556f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwarg option direction\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = ds.map(convert_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe6a266",
   "metadata": {},
   "source": [
    "### TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77290ed2-ca45-466a-ab50-1952795b9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer,max_length=256, return_tensors=\"tf\",padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1208045f-c9c1-4173-9744-77f5b3f9bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = encoded_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=['token_type_ids', 'attention_mask', 'input_ids'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "tf_validation_dataset = encoded_dataset[\"dev\"].to_tf_dataset(\n",
    "    columns=['token_type_ids', 'attention_mask', 'input_ids'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "tf_test_board_dataset = encoded_dataset[\"dev\"].to_tf_dataset(\n",
    "    columns=['token_type_ids', 'attention_mask', 'input_ids'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "tf_test_circle_dataset = encoded_dataset[\"dev\"].to_tf_dataset(\n",
    "    columns=['token_type_ids', 'attention_mask', 'input_ids'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d24c2e8f-6e46-4776-bbe2-15aa31c73b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'attention_mask': <tf.Tensor: shape=(32, 256), dtype=int64, numpy=\n",
       "   array([[1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0]])>,\n",
       "   'input_ids': <tf.Tensor: shape=(32, 256), dtype=int64, numpy=\n",
       "   array([[   2,   50, 4401, ...,    0,    0,    0],\n",
       "          [   2, 2027,   15, ...,    0,    0,    0],\n",
       "          [   2, 2278, 3095, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   2, 1802, 1953, ...,    0,    0,    0],\n",
       "          [   2, 1690, 1680, ...,    0,    0,    0],\n",
       "          [   2,   41,   41, ...,    0,    0,    0]])>,\n",
       "   'token_type_ids': <tf.Tensor: shape=(32, 256), dtype=int64, numpy=\n",
       "   array([[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]])>},\n",
       "  <tf.Tensor: shape=(32, 256), dtype=int64, numpy=\n",
       "  array([[1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0],\n",
       "         [1, 1, 1, ..., 0, 0, 0]])>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tf_train_dataset.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d9557",
   "metadata": {},
   "source": [
    "### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6b02ff3-5d64-41ba-b014-5480b4c84381",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR_RATE, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d906e177-38e7-4ff3-97e9-9d301f483e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=CKPT_PATH,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True)\n",
    "callbacks = [model_checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d389c0-5449-415d-aad6-a232418b8f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "26/26 [==============================] - 65s 2s/step - loss: 0.8112 - accuracy: 0.7709 - val_loss: 0.4758 - val_accuracy: 0.8978\n",
      "Epoch 2/15\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.3914 - accuracy: 0.8910 - val_loss: 0.2722 - val_accuracy: 0.8967\n",
      "Epoch 3/15\n",
      "26/26 [==============================] - 45s 2s/step - loss: 0.2201 - accuracy: 0.9151 - val_loss: 0.1258 - val_accuracy: 0.9854\n",
      "Epoch 4/15\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.1186 - accuracy: 0.9862 - val_loss: 0.0829 - val_accuracy: 0.9862\n",
      "Epoch 5/15\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0856 - accuracy: 0.9868 - val_loss: 0.0720 - val_accuracy: 0.9862\n",
      "Epoch 6/15\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0752 - accuracy: 0.9868 - val_loss: 0.0662 - val_accuracy: 0.9862\n",
      "Epoch 7/15\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0681 - accuracy: 0.9868 - val_loss: 0.0585 - val_accuracy: 0.9862\n",
      "Epoch 8/15\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0613 - accuracy: 0.9869 - val_loss: 0.0493 - val_accuracy: 0.9865\n",
      "Epoch 9/15\n",
      "26/26 [==============================] - 45s 2s/step - loss: 0.0523 - accuracy: 0.9872 - val_loss: 0.0397 - val_accuracy: 0.9886\n",
      "Epoch 10/15\n",
      "26/26 [==============================] - 45s 2s/step - loss: 0.0467 - accuracy: 0.9887 - val_loss: 0.0372 - val_accuracy: 0.9892\n",
      "Epoch 11/15\n",
      "26/26 [==============================] - 45s 2s/step - loss: 0.0441 - accuracy: 0.9895 - val_loss: 0.0368 - val_accuracy: 0.9894\n",
      "Epoch 12/15\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0417 - accuracy: 0.9898 - val_loss: 0.0325 - val_accuracy: 0.9895\n",
      "Epoch 13/15\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0384 - accuracy: 0.9900 - val_loss: 0.0284 - val_accuracy: 0.9897\n",
      "Epoch 14/15\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0347 - accuracy: 0.9901 - val_loss: 0.0237 - val_accuracy: 0.9899\n",
      "Epoch 15/15\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0312 - accuracy: 0.9902 - val_loss: 0.0211 - val_accuracy: 0.9901\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    " tf_train_dataset, \n",
    " epochs=EPOCHS, \n",
    " validation_data=tf_validation_dataset, \n",
    " callbacks =[callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a56c48",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION ON VALIDATION DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1a14852-bd6f-4dca-82bb-3a5a313a3fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f7d8469ff70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(CKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9671d73d-c7c6-479c-bcf3-77708ef93fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_validation_dataset_ = encoded_dataset[\"test_circle\"].to_tf_dataset(\n",
    "    columns=['token_type_ids', 'attention_mask', 'input_ids'],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1777ddef-268f-4d12-a3c6-26b475f51d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_array(x):                                        \n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2cabae8-773d-4905-ab99-4f63033efe8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 116, 116)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = []\n",
    "pred_out = []\n",
    "true_out = []\n",
    "for i in tf_validation_dataset_:\n",
    "    in_,out_ = list(i)\n",
    "    inp += list(in_['input_ids'].numpy())\n",
    "    true_out += list(out_.numpy())\n",
    "    pred = model(in_)\n",
    "    pred_out += list(np.argmax(pred['logits'].numpy(),axis=-1))\n",
    "    \n",
    "len(inp),len(true_out),len(pred_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e21541dc-28b9-4487-be50-4f845b67186f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 256)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_out = np.array(pred_out)\n",
    "pred_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6b73aa9-30b6-4b5d-b409-2775f284741f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 256)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_out = np.array(true_out)\n",
    "true_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e943494e-d92e-4477-a60a-d9d2e9b7ca22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9956331877729259"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "m = MultiLabelBinarizer().fit(true_out)\n",
    "\n",
    "f1_score(m.transform(true_out),\n",
    "         m.transform(pred_out),\n",
    "         average='macro',zero_division=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c79517",
   "metadata": {},
   "source": [
    "### ILLUSTRATION OF MODEL PREDICTING CUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6235726c-2865-4746-af6c-a8bfc3572050",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\"I dont like cake\",\"she was not diagnosed with cancer\",\"he failed to have plural effusion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc47d070-abce-4854-8d82-b7bb821d342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TokenClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c69e9b5-e231-4243-a0ae-61e19b6ae60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TokenClassificationPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd17294d-cca1-43bb-a7e0-9c7c4c619806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "out = pipe([\"May month is good\",\"she may have a lump\",\"she was not diagnosed with cancer\",\"he failed to have plural effusion\",\"her diagnosis neither suggests tumour nor indicates carcinoma\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b7250f-6bf1-43fb-9b81-781864a14dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mSample model output\u001b[0m\n",
      "********** NOTE: \u001b[32mGREEN\u001b[0m indicates Speculation, \u001b[31mRED\u001b[0m indicates Negation **********\n",
      "\n",
      "may month is good\n",
      "she may have a lump\n",
      "she was \u001b[31mnot\u001b[0m diagnosed with cancer\n",
      "he \u001b[31mfailed\u001b[0m to have plural effusion\n",
      "her diagnosis \u001b[31mneither\u001b[0m suggests tumour nor indicates carcinoma\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "print(colored(\"Sample model output\",\"blue\"))\n",
    "print(f\"{'*'*10} NOTE: {colored('GREEN','green')} indicates Speculation, {colored('RED','red')} indicates Negation {'*'*10}\")\n",
    "print()\n",
    "for j in out:\n",
    "    s = []\n",
    "    for i in j:\n",
    "        l = i['word']\n",
    "        if i['entity'] == 'LABEL_1':\n",
    "            s.append(l)\n",
    "        elif i['entity'] == 'LABEL_2':\n",
    "            s.append(colored(l,'red'))\n",
    "        elif i['entity'] == 'LABEL_3':\n",
    "            s.append(colored(l,'green'))\n",
    "    text = ' '.join([x for x in s])\n",
    "    print(text.replace(' ##',''))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f2550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
