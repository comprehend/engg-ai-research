{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9935745c",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c718c99-a9e3-42bb-bffe-723f6beb2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForTokenClassification, ElectraTokenizerFast\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import tensorflow_addons as tfa\n",
    "import math\n",
    "from transformers import TokenClassificationPipeline\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46f7cc",
   "metadata": {},
   "source": [
    "### MODEL CONFIGURATION DETAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269504b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"pretrained_models/\"\n",
    "MODEL_PATH = BASE_PATH + \"NegBioElectra/\"\n",
    "LR_RATE = 5.5e-5\n",
    "CKPT_PATH = BASE_PATH + \"sherlock_models/NegBioElectra_sherlock_scope_model\"\n",
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e809d142",
   "metadata": {},
   "source": [
    "### LOAD BASE MODEL & TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b62dd0d-d4a7-4e4e-adb4-d397845b6944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraForTokenClassification: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFElectraForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFElectraForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_PATH, num_labels=1,from_pt=True\n",
    ")\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(MODEL_PATH,max_length=256)\n",
    "tokenizer.add_tokens(['[NEG]'], special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06b82c",
   "metadata": {},
   "source": [
    "### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b1b54a-06cd-4f0d-97a8-626e7d3c98f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-68d72f5d816b845f\n",
      "Reusing dataset csv (/home/studio-lab-user/.cache/huggingface/datasets/csv/default-68d72f5d816b845f/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cdc60276ed432494a251fa7ad9b58c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'labels'],\n",
       "        num_rows: 847\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['sentence', 'labels'],\n",
       "        num_rows: 144\n",
       "    })\n",
       "    test_card: Dataset({\n",
       "        features: ['sentence', 'labels'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "    test_circle: Dataset({\n",
       "        features: ['sentence', 'labels'],\n",
       "        num_rows: 116\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"csv\", data_files={'train': 'data/sherlock_scope_train.csv','dev':'data/sherlock_scope_dev.csv',\"test_card\":\"data/sherlock_scope_test_cardboard.csv\",\"test_circle\":\"data/sherlock_scope_test_circle.csv\"})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dae54d",
   "metadata": {},
   "source": [
    "### PRE-PROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05444567-8e92-4032-910f-e06f8d6b0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(examples):\n",
    "    l = {'labels':[list(np.array(i.split(\"|\"),dtype=\"int32\")) for i in examples['labels']]} \n",
    "    l.update(tokenizer(examples['sentence'],truncation=True,max_length=256,padding=True))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0324854f-05d8-4611-b404-96ee7f89b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/csv/default-68d72f5d816b845f/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-cb5576f249310cd3.arrow\n",
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/csv/default-68d72f5d816b845f/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-39b9ddc55b807479.arrow\n",
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/csv/default-68d72f5d816b845f/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-ee328fe82e487357.arrow\n",
      "Loading cached processed dataset at /home/studio-lab-user/.cache/huggingface/datasets/csv/default-68d72f5d816b845f/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-90f2f2a0fb3734b3.arrow\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = ds.map(convert_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9ab43",
   "metadata": {},
   "source": [
    "### TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65af0177-65f8-44c1-bf70-cc052320297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer,max_length=256, return_tensors=\"tf\",padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eb8b641-73a1-4fae-ae06-8a2ac1d595ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf_train_dataset = encoded_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=['token_type_ids', 'attention_mask', 'input_ids'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "tf_validation_dataset = encoded_dataset[\"dev\"].to_tf_dataset(\n",
    "    columns=['token_type_ids', 'attention_mask', 'input_ids'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac0a73e-4e2a-442a-bbd4-dc423783887a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'attention_mask': <tf.Tensor: shape=(32, 256), dtype=int64, numpy=\n",
       "   array([[1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0],\n",
       "          [1, 1, 1, ..., 0, 0, 0]])>,\n",
       "   'input_ids': <tf.Tensor: shape=(32, 256), dtype=int64, numpy=\n",
       "   array([[   2, 2493, 2963, ...,    0,    0,    0],\n",
       "          [   2,   41,   41, ...,    0,    0,    0],\n",
       "          [   2,   41,   41, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   2, 1680, 3464, ...,    0,    0,    0],\n",
       "          [   2, 1690, 4728, ...,    0,    0,    0],\n",
       "          [   2,   41,   41, ...,    0,    0,    0]])>,\n",
       "   'token_type_ids': <tf.Tensor: shape=(32, 256), dtype=int64, numpy=\n",
       "   array([[0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0],\n",
       "          [0, 0, 0, ..., 0, 0, 0]])>},\n",
       "  <tf.Tensor: shape=(32, 256), dtype=int64, numpy=\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]])>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tf_train_dataset.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965a4d5",
   "metadata": {},
   "source": [
    "### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68b45b2c-7112-4932-9c76-f4505fce50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=LR_RATE, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d999e7b7-f89b-4df4-8899-60520687fc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=CKPT_PATH,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True)\n",
    "callbacks = [model_checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "427402d0-5957-43b1-9439-59a7e9a869f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 64s 2s/step - loss: 0.2750 - accuracy: 0.9684 - val_loss: 0.1569 - val_accuracy: 0.9639\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.1410 - accuracy: 0.9680 - val_loss: 0.1495 - val_accuracy: 0.9651\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 43s 2s/step - loss: 0.1100 - accuracy: 0.9680 - val_loss: 0.0714 - val_accuracy: 0.9641\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0671 - accuracy: 0.9679 - val_loss: 0.0534 - val_accuracy: 0.9659\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0451 - accuracy: 0.9814 - val_loss: 0.0425 - val_accuracy: 0.9844\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0319 - accuracy: 0.9901 - val_loss: 0.0358 - val_accuracy: 0.9888\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0216 - accuracy: 0.9943 - val_loss: 0.0331 - val_accuracy: 0.9892\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0148 - accuracy: 0.9964 - val_loss: 0.0318 - val_accuracy: 0.9906\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0098 - accuracy: 0.9982 - val_loss: 0.0357 - val_accuracy: 0.9908\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 45s 2s/step - loss: 0.0070 - accuracy: 0.9990 - val_loss: 0.0319 - val_accuracy: 0.9921\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0056 - accuracy: 0.9993 - val_loss: 0.0366 - val_accuracy: 0.9918\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 44s 2s/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 0.0335 - val_accuracy: 0.9925\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.0424 - val_accuracy: 0.9908\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.0404 - val_accuracy: 0.9914\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 43s 2s/step - loss: 0.0036 - accuracy: 0.9996 - val_loss: 0.0481 - val_accuracy: 0.9901\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 45s 2s/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.0314 - val_accuracy: 0.9928\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 45s 2s/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.0321 - val_accuracy: 0.9932\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.0373 - val_accuracy: 0.9924\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.0351 - val_accuracy: 0.9931\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.0388 - val_accuracy: 0.9922\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.0369 - val_accuracy: 0.9923\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0396 - val_accuracy: 0.9921\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 43s 2s/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.0316 - val_accuracy: 0.9937\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 41s 2s/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.0394 - val_accuracy: 0.9930\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 43s 2s/step - loss: 0.0015 - accuracy: 0.9999 - val_loss: 0.0343 - val_accuracy: 0.9938\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.0440 - val_accuracy: 0.9922\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.0502 - val_accuracy: 0.9909\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 42s 2s/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0451 - val_accuracy: 0.9918\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 43s 2s/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 0.0392 - val_accuracy: 0.9929\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 45s 2s/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.0304 - val_accuracy: 0.9942\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    " tf_train_dataset, \n",
    " epochs=30,\n",
    " validation_data=tf_validation_dataset,\n",
    " callbacks =[callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd5b474",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION ON TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f33cba2-e210-4954-8f40-b12ebc41807d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f80d9dd2df0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(CKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7e279bf-3e7c-4472-8de6-e10e15ee85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_array(x):                                        \n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3c07a7c-fc1a-415f-8ec4-c50c3ee3f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test_circle = encoded_dataset[\"test_circle\"].to_tf_dataset(\n",
    "    columns=['token_type_ids', 'attention_mask', 'input_ids'],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "tf_test_cardboard = encoded_dataset[\"test_card\"].to_tf_dataset(\n",
    "    columns=['token_type_ids', 'attention_mask', 'input_ids'],\n",
    "    label_cols=[\"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6347c234-960e-4098-83e5-a4eae00e880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119, 119, 119)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = []\n",
    "pred_out = []\n",
    "true_out = []\n",
    "for i in tf_test_cardboard:\n",
    "    in_,out_ = list(i)\n",
    "    inp += list(in_['input_ids'].numpy())\n",
    "    true_out += list(out_.numpy())\n",
    "    pred = model(in_)\n",
    "    pred_out += list(sigmoid_array(pred['logits'].numpy()))\n",
    "    \n",
    "len(inp),len(true_out),len(pred_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b244b609-696d-4ce8-bdae-d5b04b5c96f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 235, 235)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tf_test_circle:\n",
    "    in_,out_ = list(i)\n",
    "    inp += list(in_['input_ids'].numpy())\n",
    "    true_out += list(out_.numpy())\n",
    "    pred = model(in_)\n",
    "    pred_out += list(sigmoid_array(pred['logits'].numpy()))\n",
    "    \n",
    "len(inp),len(true_out),len(pred_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "359c0607-fc2c-4834-89f1-32fd95ca92a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 256, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_out = np.array(pred_out)\n",
    "pred_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f39b3b2-dab0-4b1f-8bc2-0e48e1026c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 256)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_out = np.reshape(pred_out,(pred_out.shape[0],256))\n",
    "pred_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c608a7a-c6fe-4c25-9fa9-1b681c49a50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_out = np.round(pred_out)\n",
    "pred_out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3763424d-ebd5-417a-9535-19d02616f4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(235, 256)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_out = np.array(true_out)\n",
    "true_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b948a13e-ab3d-4eca-bbd6-469cbfdac766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9726279760174308"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(true_out, pred_out, average='macro',zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df823af-1d9a-490a-8d28-2d528ecdb559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
